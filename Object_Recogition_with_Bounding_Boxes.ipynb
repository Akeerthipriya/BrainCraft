{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mobilenetv2 + SSDLite",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DxS8o4HaO9r",
        "colab_type": "toc"
      },
      "source": [
        ">[Downloading Model Data](#scrollTo=m_Vt_gHBOjxd)\n",
        "\n",
        ">[Configuring TensorFlow](#scrollTo=5IjH5gusOhy9)\n",
        "\n",
        ">[Webcam Demo](#scrollTo=L95kdZi1QDFw)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Vt_gHBOjxd",
        "colab_type": "text"
      },
      "source": [
        "# Downloading Model Data\n",
        "\n",
        "Run these steps first to download the TensorFlow model data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIZ1ofUKOdhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/tensorflow/models\n",
        "checkpoint_name = 'ssdlite_mobilenet_v2_coco_2018_05_09'\n",
        "!wget http://download.tensorflow.org/models/object_detection/{checkpoint_name}.tar.gz\n",
        "!tar -xf {checkpoint_name}.tar.gz\n",
        "checkpoint = '{0}.ckpt'.format(checkpoint_name)\n",
        "!cd /content/models/research && protoc object_detection/protos/*.proto --python_out=.\n",
        "print('Setup successful!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5IjH5gusOhy9",
        "colab_type": "text"
      },
      "source": [
        "# Configuring TensorFlow\n",
        "\n",
        "These steps start TensorFlow and read the downloaded model data into memory so we can use them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22G9YlIIPxZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NOTE: Pieces taken from https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\n",
        "# The github.com/tensorflow/models is distributed under the Apache 2.0 license.\n",
        "\n",
        "import numpy\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.reset_default_graph()\n",
        "!rm -f /content/logs/*\n",
        "\n",
        "with tf.get_default_graph().as_default() as graph:  \n",
        "  summary_writer = tf.summary.FileWriter(\"/content/logs/\", flush_secs=1)\n",
        "\n",
        "  # This section builds a \"graph\" in TensorFlow to explain how to process the data.\n",
        "  jpeg_input_tensor = tf.placeholder(tf.string, ())  # We will provide a JPEG to TF.\n",
        "\n",
        "  # First, instruct TF to decode the JPEG string into a matrix.\n",
        "  image = tf.image.decode_image(jpeg_input_tensor)\n",
        "  image_tensor = tf.expand_dims(image, 0)\n",
        "\n",
        "  # Load the Mobilenetv2 + SSDLite graph from disk.\n",
        "  ssdlite_graph = tf.GraphDef()\n",
        "  with open('{}/frozen_inference_graph.pb'.format(checkpoint_name), 'rb') as f:\n",
        "    ssdlite_graph.ParseFromString(f.read())\n",
        "    \n",
        "  # Tell TensorFlow we would like to inspect these parts of the network.\n",
        "  output_names = ['num_detections:0', \n",
        "                  'detection_boxes:0', \n",
        "                  'detection_scores:0',\n",
        "                  'detection_classes:0']\n",
        "  ops = dict(zip(output_names, tf.graph_util.import_graph_def(\n",
        "      ssdlite_graph, \n",
        "      input_map={'image_tensor': image_tensor},\n",
        "      return_elements=output_names)))\n",
        "    \n",
        "  # Also extract the decoded image from the network to draw bounding boxes.\n",
        "  ops['image'] = image\n",
        "  \n",
        "  summary_writer.add_graph(graph)\n",
        "  summary_writer.flush()\n",
        "\n",
        "\n",
        "def run_detection(sess, img):\n",
        "  \"\"\"Run one detection round.\"\"\"\n",
        "  output_dict = sess.run(ops, feed_dict={jpeg_input_tensor: img})\n",
        "\n",
        "  # all outputs are float32 numpy arrays, so convert types as appropriate\n",
        "  output_dict['num_detections'] = int(output_dict['num_detections:0'][0])\n",
        "  output_dict['detection_classes'] = output_dict[\n",
        "      'detection_classes:0'][0].astype(numpy.int64)\n",
        "  output_dict['detection_boxes'] = output_dict['detection_boxes:0'][0]\n",
        "  output_dict['detection_scores'] = output_dict['detection_scores:0'][0]\n",
        "  \n",
        "  return output_dict\n",
        "\n",
        "print('Model configured')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmiF7JM8-KnC",
        "colab_type": "text"
      },
      "source": [
        "## Optional: Visualize the Graph with TensorBoard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMtIkp7QBR6D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "!mkdir /content/logs\n",
        "!ps ax | grep tensorboard | awk '{print $1}' | xargs kill\n",
        "%tensorboard --logdir /content/logs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L95kdZi1QDFw",
        "colab_type": "text"
      },
      "source": [
        "# Demo\n",
        "\n",
        "This section creates the video input element and connects it to TensorFlow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbaKfbOxQCwi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import base64\n",
        "import html\n",
        "import io\n",
        "import time\n",
        "\n",
        "# setup path so that mobilenet_v2 can be found.\n",
        "import sys\n",
        "sys.path.append('/content/models/research')\n",
        "from object_detection.utils import label_map_util\n",
        "from object_detection.utils import visualization_utils\n",
        "\n",
        "# Taken from https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb#scrollTo=SucxddsPhOmj\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "import numpy\n",
        "import PIL.Image\n",
        "\n",
        "def start_input():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 512, 512);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 512; //video.videoWidth;\n",
        "      captureCanvas.height = 512; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function takePhoto(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def take_photo(label, img_data):\n",
        "  data = eval_js('takePhoto(\"{}\", \"{}\")'.format(label, img_data))\n",
        "  return data\n",
        "\n",
        "category_index = label_map_util.create_category_index_from_labelmap(\n",
        "    '/content/models/research/object_detection/data/mscoco_label_map.pbtxt',\n",
        "    use_display_name=True)\n",
        "    \n",
        "with tf.Session() as sess:\n",
        "  start_input()\n",
        "\n",
        "  label_html = 'Capturing...'\n",
        "  img_data = ''\n",
        "  while True:\n",
        "    capture_start = time.time()\n",
        "    js_reply = take_photo(label_html, img_data)\n",
        "    capture_end = time.time()\n",
        "    if not js_reply:\n",
        "      break\n",
        "\n",
        "    # Javascript returns a data URL, like:\n",
        "    #     data: image/jpeg;base64,<base-64 encoded data>\n",
        "    # To use the image, decode the base-64 encoded part and treat it as a JPEG.\n",
        "    jpeg_input = base64.b64decode(js_reply['img'].split(',')[1])\n",
        "    result = run_detection(sess, jpeg_input)\n",
        "    detect_end = time.time()\n",
        "    \n",
        "    # To reduce transfer sizes, we send just the bounding boxes drawn on a \n",
        "    # transparent PNG. Here, we create a blank PNG.\n",
        "    rgb_shape = result['image'].shape\n",
        "    rgba_shape = list(rgb_shape)[0:2] + [4]\n",
        "    image_np = numpy.zeros(rgba_shape, dtype=numpy.uint8)\n",
        "    \n",
        "    # Draw the bounding boxes in the RGB channels.\n",
        "    visualization_utils.visualize_boxes_and_labels_on_image_array(\n",
        "      image_np[:, :, 0:3],  # sub-select RGB channels only; alpha is done below.\n",
        "      result['detection_boxes'],\n",
        "      result['detection_classes'],\n",
        "      result['detection_scores'],\n",
        "      category_index,\n",
        "      instance_masks=result.get('detection_masks'),\n",
        "      use_normalized_coordinates=True,\n",
        "      line_thickness=8)\n",
        "    \n",
        "    # To be visible, the alpha channel also needs to be edited. Set the alpha\n",
        "    # channel to 255 (fully opaque) wherever anything was drawn.\n",
        "    image_t = image_np.transpose()\n",
        "    max_color = numpy.maximum(numpy.maximum(image_t[0], image_t[1]), image_t[2])\n",
        "    image_t[3] = numpy.clip(max_color, 0, 1) * 255\n",
        "    viz_end = time.time()\n",
        "\n",
        "    # Save the image as a PNG in memory and assemble a data URL.\n",
        "    im = PIL.Image.fromarray(image_np, 'RGBA')\n",
        "    iobuf = io.BytesIO()\n",
        "    im.save(iobuf, format='png')\n",
        "    img_data = 'data:image/png;base64,{}'.format(\n",
        "      (str(base64.b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "    perf_measures = {\n",
        "        'server': (\n",
        "            ('take_photo', capture_end - capture_start),\n",
        "            ('run_detection', detect_end - capture_end),\n",
        "            ('visualize', viz_end - detect_end)\n",
        "        ),\n",
        "        'js': (\n",
        "            ('create', js_reply['create']),\n",
        "            ('show', js_reply['show']),\n",
        "            ('capture', js_reply['capture']),\n",
        "        ),\n",
        "    }\n",
        "    \n",
        "    label_text = 'img size: {}b\\ntime:\\n  server: {}\\n  js: {}'.format(\n",
        "        len(js_reply['img']),\n",
        "        ', '.join('{}: {:2.3f}s'.format(*x) for x in perf_measures['server']),\n",
        "        ', '.join('{}: {:2.3f}s'.format(x[0], x[1] / 1000) for x in perf_measures['js']),\n",
        "    )\n",
        "    \n",
        "    label_html = html.escape(label_text).replace('\\n', '<br/>')\n",
        "\n",
        "print('Finished')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}